{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from warnings import filterwarnings\n",
        "from os import getcwd\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "from dspf.factor_analysis import FactorAnalysis\n",
        "from dspf.factor_analysis.plotting import scree_plot, create_loadings_heatmaps\n",
        "from factor_analyzer.factor_analyzer import calculate_kmo\n",
        "\n",
        "filterwarnings(\"ignore\")\n",
        "np.set_printoptions(suppress=True, precision=4)\n",
        "pd.set_option(\"display.precision\", 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Case Study: California Housing Data (1990)\n",
        "\n",
        "Each observation (row) in the dataset represents what is called a [*block group*](https://www.census.gov/programs-surveys/geography/about/glossary.html#par_textimage_4) in California.\n",
        "\n",
        "A 'block group' is a statistical division from the U.S. Census Bureau,\n",
        "which should include between 600 and 3000 people.\n",
        "Instead of block group, we will mostly use the term *district* here for simplicity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## The data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "path = Path(getcwd(), \"data\", \"cal_housing.data\")\n",
        "X = pd.read_csv(path, header=0, sep=\",\")\n",
        "X.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "In total, we have 20640 observations (districts) and 9 metric scaled features.\n",
        "\n",
        "Since we also need metrically scaled features for the factor analysis, we have no problems in this area."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "X.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The features can thus be described as follows:\n",
        "\n",
        "- `longitude`: longitude of a district, values between -124.35 and -114.31\n",
        "- latitude`: latitude of a district, values between -32.54 and 41.95\n",
        "- `housing_median_age`: median age of houses in a district, values between 1 and 52 (years)\n",
        "- `total_rooms`: total number of rooms in a district, values between 2 and 39320\n",
        "- `total_bedrooms`: total number of bedrooms in a district, values between 1 and 6445\n",
        "- population`: total number of inhabitants of a district, values between 3 and 35682\n",
        "- `households`: total number of households in a district, values between 1 and 6082\n",
        "- `median_income`: median income of the inhabitants, values between 0.5 and 15\n",
        "- median_house_value`: median of the monetary value of the houses, values between 15000 and 500 001 dollars.\n",
        "\n",
        "The range of values of `median_income` is particularly striking. This is probably an adjusted scale and not income in dollars.\n",
        "\n",
        "If we look again at the description of a district, we see that in this data set there are\n",
        "there might be some outliers in terms of `population`. Because a district here should be between 600 and 3000 people."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Missing values\n",
        "\n",
        "The dataset does not contain any missing values, as we can see in the `pd.info` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## The distribution of the data\n",
        "\n",
        "Now let's look at some univariate and bivariate plots of the metric features to get a feel for the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "X.hist(figsize=(20, 15), bins=30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "\n",
        "Here we see that the four features `total_rooms`, `total_bedrooms`, `population` and `households` have a relatively high\n",
        "skewness.\n",
        "Similarly, `median_house_value` has a high density at circa 500 000. This could be due to the fact\n",
        "that the value 500 000 was used as an upper bound.\n",
        "\n",
        "\n",
        "### Outlier Analysis\n",
        "So before we look at bivariate plots, we will use LocalOutlierFactor (LOF) to try to eliminate some outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "labels = LocalOutlierFactor(n_neighbors=35).fit_predict(X)\n",
        "outliers = X[labels == -1]\n",
        "X = X[labels == 1]\n",
        "\n",
        "print(f\"Number of districts identified as outliers by means of LOF: {outliers.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "X.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We can see that the maximum value of 'population' is now significantly lower. Nevertheless, there are districts with very small populations (minimum 3 inhabitants).\n",
        "\n",
        "Let us now take a closer look at the distributions of the features by means of a pairplot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "sns.pairplot(X, diag_kind=\"kde\", kind=\"scatter\", plot_kws={\"alpha\": 0.3})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "In this pairplot, univariate plots are on the main diagonal and bivariate plots are on the off-diagonal elements.\n",
        "Three groups in particular stand out here:\n",
        "\n",
        "##### Group 1\n",
        "The features `longitude` and `latitude` show a relatively high negative correlation.\n",
        "\n",
        "\n",
        "##### Group 2\n",
        "The features `total_rooms`, `total_bedrooms`, `population` and `households` are highly correlated with each other. However, this is\n",
        "not surprising considering the meaning of the features, since a high number of people in the district must eventually mean that more\n",
        "rooms exist and overall the number of households must probably be higher.\n",
        "\n",
        "The feature `housing_median_age` could also still be included in this group, as it shows a slightly negative correlation with these four features.\n",
        "\n",
        "##### Group 3\n",
        "The features `median_income` and `median_house_value` also show a visible positive correlation, but not as strong as those in group 2.\n",
        "\n",
        "These three groups are shown again below in separate pairplots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "group1 = [\"longitude\", \"latitude\"]\n",
        "group2 = [\"total_rooms\", \"total_bedrooms\", \"population\", \"households\"]\n",
        "group3 = [\"median_income\", \"median_house_value\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "sns.pairplot(X[group1], diag_kind=\"kde\", kind=\"scatter\", plot_kws={\"alpha\": 0.1}, size=4)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Here we see that very many districts are located in two regions. This explains the bimodal structure of the two features 'longitude' and 'latitude'.\n",
        "Below is a scatter plot that shows the geographic distribution of the districts in more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "sns.pairplot(X[group2], diag_kind=\"kde\", kind=\"scatter\", plot_kws={\"alpha\": 0.3}, size=3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "sns.pairplot(X[group3], diag_kind=\"kde\", kind=\"scatter\", plot_kws={\"alpha\": 0.3}, size=4)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "In the last pairplot, the horizontal (or vertical) line at `median_house_value = 500 000` stands out again.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### Geographical distribution of house prices\n",
        "\n",
        "In addition, we can still look at the graphical distribution of house prices.\n",
        "Shown below is a scatter plot, which takes into account the location of the districts in dependence\n",
        "of the median house prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "sc = ax.scatter(x=\"longitude\", y=\"latitude\", c=\"median_house_value\", cmap=\"Blues\", data=X, alpha=0.4)\n",
        "plt.colorbar(sc, label=\"median_house_value\")\n",
        "\n",
        "cities = [\"Los Angeles\", \"San Francisco\"]\n",
        "xys = [(-118.243683, 34.052235), (-122.431297, 37.773972)]\n",
        "xys_text = [(-121, 33.5), (-123, 35.5)]\n",
        "for city, xy, xytext in zip(cities, xys, xys_text):\n",
        "    ax.annotate(city, xy=xy,  xycoords='data',\n",
        "            xytext=xytext,\n",
        "            arrowprops=dict(facecolor='red', shrink=0.05),\n",
        "            horizontalalignment='right', verticalalignment='top',\n",
        "            )\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "ax.set_xlabel(\"Longitude\")\n",
        "ax.set_ylabel(\"Latitude\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The districts with very expensive houses are at the bottom, that is, on the coast. Two large cities are marked with an arrow in the picture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Factor Analysis Step 1: Examine the Suitability of the Data\n",
        "\n",
        "First, we need to examine the suitability of the data for factor analysis.\n",
        "For this purpose, we use the Kaiser-Meyer-Olkin criterion (KMO criterion) for the complete data set and the related\n",
        "related Measure of sampling adequacy (MSA) for each individual feature.\n",
        "\n",
        "In general, we want the KMO value to be above 0.5 and the MSA value for each variable not to be below 0.5.\n",
        "below 0.5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "msa_values, kmo = calculate_kmo(X)\n",
        "print(f\"The KMO value is {kmo:.4f}\\n\")\n",
        "msa_df = pd.DataFrame(msa_values.reshape(-1, 1), index=X.columns, columns=[\"MSA\"])\n",
        "print(msa_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The KMO value is above 0.6, which indicates acceptable quality. However, this is not an ideal value.\n",
        "Also, the MSA value for 4 variables is below 0.5, but the value for the other features is good.\n",
        "\n",
        "Now, for example, one could remove the features with an MSA value below 0.5. For this example\n",
        "we will continue with all variables.\n",
        "We should also look directly at the correlation matrix. We do this in the following with a\n",
        "heatmap, since it is very well suited as a visual representation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 10))\n",
        "corr = X.corr().round(2)\n",
        "hm = sns.heatmap(data=corr, vmax=1, vmin=-1, cmap=\"RdBu_r\", annot=True)\n",
        "hm.set_xticklabels(X.columns, rotation=45)\n",
        "hm.set_yticklabels(X.columns)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "As we have already seen in the pairplot, there are three groups of highly correlated features:\n",
        "The four features in the middle, and `housing_median_age` with a slightly negative correlation to these four features.\n",
        "Similarly, `longitude` and `latitude` are negatively correlated with each other. The features `median_income` and `median_house_value` are positively correlated with each other.\n",
        "\n",
        "This already suggests that a 3-factor solution might probably be a good choice of k. In the following, we will look at this in more detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Factor Analysis Step 2: Choosing the Number of Factors.\n",
        "\n",
        "To answer this question, we will first extract all factors using the Principal Component (PC) method and consider the eigenvalues of the factors using a scree plot.\n",
        "and look at the eigenvalues of the factors using a scree plot.\n",
        "\n",
        "Then we can keep the factors that have an eigenvalue greater than one (Kaiser criterion), or use a 'kink' in the plot to identify a suitable factor number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "fa = FactorAnalysis(n_factors=X.shape[1], method=\"pc\").fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(7, 5))\n",
        "scree_plot(fa.eigenvalues_, ax)\n",
        "ax.set_xlabel(\"Factor\")\n",
        "ax.set_ylabel(\"Eigenvalue\")\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can see that the first three factors have an eigenvalue\n",
        "greater than one. This makes sense with respect to the correlation matrix,\n",
        "which we saw earlier, because of the three 'boxes'. According to\n",
        "the Kaiser criterion, we should therefore use a 3-factor model.\n",
        "\n",
        "The fourth factor, however, is only minimally below the eigenvalue of one, which is why\n",
        "one should not exclude it directly. A 'kink' would be visible at\n",
        "`k = 5` recognizable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Factor analysis Step 3: Extracting the factors\n",
        "Now we will perform the actual factor extraction.\n",
        "\n",
        "For this we will compare three different extraction methods:\n",
        " - Principal Components (PC) Method.\n",
        " - Principal Axis Factoring (PAF) Method\n",
        " - Iterated Principal Axis Factoring (Iterated PAF)\n",
        "\n",
        "The last variant is probably the most frequently used method (among these three).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "methods = [\n",
        "    (\"PC\", FactorAnalysis(method=\"pc\")),\n",
        "    (\"Non-iterated PAF\", FactorAnalysis(method=\"paf\", max_iter=1)),\n",
        "    (\"Iterated PAF\", FactorAnalysis(method=\"paf\", max_iter=50))\n",
        "]\n",
        "for n_factors in range(3, 6):\n",
        "    figsize = (10 + (1+n_factors)//2, 8)\n",
        "    create_loadings_heatmaps(X, methods, figsize, fa_params={\"n_factors\": n_factors})\n",
        "    plt.gcf().suptitle(f\"Loadings matrices of a {n_factors}-factor-model\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Here we see in each row the charge matrices of the three different methods shown as a heat map.\n",
        "We note that the 3-factor solution is indeed a good solution in terms of a simple structure.\n",
        "However, the 4-factor solution could still be a valid solution as well. Only the 5-factor solution is problematic,\n",
        "since no feature loads high (absolute value greater than 0.5) on the fifth factor.\n",
        "\n",
        "One problem, however, is that the feature `housing_median_age` loads very high on the fourth factor only with the PC method\n",
        "and has only a moderate loading on the first factor for a 3-factor solution.\n",
        "This indicates a high specific variance, i.e., the factors are not well able to explain the variance of this feature\n",
        "explain the variance of this feature.\n",
        "\n",
        "So we will look at the 3-factor solution in more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "fa_params = {\"n_factors\": 3}\n",
        "\n",
        "axes = create_loadings_heatmaps(X, methods, figsize=(10, 9), fa_params=fa_params, annotate=True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Wir können sehen, dass bei der PC-Methode die Ladungen leicht höher ausfallen und dass der zweite Faktor ein unterschiedliches\n",
        "Vorzeichen besitzt, im Gegensatz zu den anderen beiden Methoden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The iterated and non-iterated PAF methods are very similar to each other. However, the iterated\n",
        "variant is often better in terms of the reproduced correlation matrix. This can be seen from the\n",
        "*root mean squared error* (RMSE):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "for method, fa in methods:\n",
        "    rmse = fa.get_rmse()\n",
        "    print(f\"RMSE of {method}: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Der root mean squared error of residuals (RMSE) ist bei der iterierten PAF-Methode am geringsten,\n",
        "gefolgt von der nicht-iterativen Variante und der Hauptkomponentenmethode.\n",
        "\n",
        "Schauen wir uns nun noch die Kommunalitäten der Variablen an. Dies zeigt uns zum Beispiel die `print_summary` Methode an:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Zusammenfassung der iterierten PAF-Methode\n",
        "methods[2][1].print_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The specific variance of `housing_median_age` is very high with a value of 0.8980.\n",
        "This means that the factors together cannot explain the variance of this characteristic very well.\n",
        "This is also reflected in the low loadings on the three factors.\n",
        "However, the specific variances on the remaining features are very low,\n",
        "which is a good sign for the quality of the factor solution.\n",
        "\n",
        "\n",
        "Since the feature `housing_median_age` has a very high specific variance (low commonality)\n",
        "we can also look at a 3-factor model without this feature.\n",
        "This can reduce the RMSE by almost 44% (in comparison are the iterated PAF methods),\n",
        "so we remove this feature for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "X_without_age = X.drop(columns=\"housing_median_age\", axis=1)\n",
        "fa_without_age = FactorAnalysis(n_factors=3).fit(X_without_age)\n",
        "perc = 1 - fa_without_age.get_rmse() / rmse\n",
        "print(f\"The RMSE could be reduced by {perc:.2%} by removing the feature\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "\n",
        "We can also analyze the difference between the methods in the RMSE graphically:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(ncols=2, figsize=(12, 6))\n",
        "fitted_methods = [\n",
        "    (\"PC\", FactorAnalysis(method=\"pc\", n_factors=3).fit(X)),\n",
        "    (\"Iterated PAF\", FactorAnalysis(method=\"paf\", n_factors=3, max_iter=50).fit(X))\n",
        "]\n",
        "for ax, (method, fa) in zip(axes, fitted_methods):\n",
        "    R = fa.corr_\n",
        "    R_hat = fa.get_reprod_corr()\n",
        "    abs_residuals = np.abs(R - R_hat)\n",
        "    mask = np.triu(np.ones_like(R))\n",
        "    ax.set_title(f\"{method} (RMSE = {fa.get_rmse():.4f})\", fontsize=11)\n",
        "    s = sns.heatmap(abs_residuals.round(2), cmap=\"BuGn\", ax=ax, cbar=False, annot=True, square=True, mask=mask)\n",
        "    s.set_xticklabels(range(1, 10))\n",
        "    s.set_yticklabels(range(1, 10), rotation=0)\n",
        "fig.suptitle(\"Residual matrices of two extraction methods with k=3\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Before we move on to factor rotation and interpretation, we will compare the different initial estimates of the communalities in the (iterated) PAF method.\n",
        "It might be interesting to see whether the choice of the initial estimate has an influence on the final loadings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "paf_comparison_methods = [\n",
        "    (\"Non-iterated PAF\", FactorAnalysis(method=\"paf\", max_iter=1)),\n",
        "    (\"PAF with max_iter=3\", FactorAnalysis(method=\"paf\", max_iter=3)),\n",
        "    (\"Iterated PAF\", FactorAnalysis(method=\"paf\", max_iter=50)),\n",
        "]\n",
        "figsize = (8, 6)\n",
        "initial_communality_estimates = {\n",
        "    \"smc\": \"Squared multiple correlations (SMC)\",\n",
        "    \"mac\": \"Maximum absolute correlation (MAC)\",\n",
        "    \"ones\": \"Ones\"\n",
        "}\n",
        "for init_comm in initial_communality_estimates:\n",
        "    print(f\"Initial Estimate: {initial_communality_estimates[init_comm]}\")\n",
        "    create_loadings_heatmaps(X, paf_comparison_methods, figsize, fa_params={\"n_factors\": 3, \"initial_comm\" : init_comm})\n",
        "    plt.show()\n",
        "    print(f\"Iterated PAF finished in {paf_comparison_methods[2][1].n_iter_} Iterations.\")\n",
        "    for method, fa in paf_comparison_methods:\n",
        "        print(f\"RMSE of {method}: {fa.get_rmse():.4f}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "We can see that only small differences between the different initial estimates are\n",
        "in the charges are detectable. Only in the non-iterated variant of the PAF method can we detect some differences, especially in the second factor.\n",
        "differences, especially in the second factor. For example, the second factor here has a different\n",
        "sign, but only when ones were used as the communality estimate. In this\n",
        "case, the result is identical to the principal component method.\n",
        "\n",
        "We note, however, that the iterated variant requires a different number of iterations until\n",
        "the convergence criterion is reached. On this data set, it is slowest in the case of ones (10 iterations) and fastest\n",
        "in the case of maximum absolute correlations (MAC) (only 3 iterations). However, if the convergence criterion is met, the loadings are\n",
        "are largely identical for all three initial estimates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Factor Analysis Step 4: Factor Rotation and Interpretation\n",
        "Now we rotate the loadings with the Varimax method and try to interpret the factors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "methods = [\n",
        "    (\"Unrotated\", FactorAnalysis(method=\"paf\", rotation=None)),\n",
        "    (\"Varimax-Rotation\", FactorAnalysis(method=\"paf\", rotation=\"varimax\"))\n",
        "]\n",
        "fa_params = {\"n_factors\": 3}\n",
        "fig = create_loadings_heatmaps(X_without_age, methods, fa_params=fa_params)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "If we had to interpret the factors, we could say that\n",
        "- the first factor reflects the size of the district,\n",
        "- the second factor takes into account the location of the district, and\n",
        "- the third factor denotes the prosperity in the district.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Factor analysis Step 5: Determine the factor values\n",
        "\n",
        "As a final step, we can take a look at the estimated factor scores\n",
        ". As the district `i`, how would it have rated the three factors described above?\n",
        "\n",
        "The method used here to estimate the factor scores is the *regression method*,\n",
        "which uses multivariate linear regression to estimate the factor scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "scores = FactorAnalysis(n_factors=3).fit_transform(X_without_age)\n",
        "scores = pd.DataFrame(scores, columns=[\"Size\", \"Location\", \"Prosperity\"])\n",
        "scores.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "scores.std(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "Here, the factor scores do not have a unit variance or standard deviation of one because the PAF method with\n",
        "squared multiple squares was used as the initial estimate.\n",
        "\n",
        "However, if the principal component method is used, the factor scores have a standard deviation of one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "scores = FactorAnalysis(method=\"pc\", n_factors=3).fit_transform(X_without_age)\n",
        "scores.std(axis=0)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.6 ('.venv': poetry)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "1dfa5ff1a58f8ea3949ba1ba121eb267c6175f1970afa18aad6cc74461107d6b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
